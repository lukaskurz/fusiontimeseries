{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Load Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_files(benchmark_dir: Path) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Load all benchmark JSON files from the directory.\"\"\"\n",
    "    benchmarks = {}\n",
    "\n",
    "    for json_file in benchmark_dir.glob(\"*.json\"):\n",
    "        with open(json_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            model_name = data[\"model\"]\n",
    "\n",
    "            # Store multiple runs for the same model\n",
    "            if model_name not in benchmarks:\n",
    "                benchmarks[model_name] = []\n",
    "            benchmarks[model_name].append(data)\n",
    "\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "# Load all benchmarks\n",
    "benchmark_dir = Path(\"benchmarks\")\n",
    "all_benchmarks = load_benchmark_files(benchmark_dir)\n",
    "\n",
    "print(f\"üìä Loaded benchmarks for {len(all_benchmarks)} models:\")\n",
    "for model, runs in all_benchmarks.items():\n",
    "    print(f\"  - {model}: {len(runs)} run(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Data Processing and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_benchmark_data(benchmarks: Dict[str, List[Dict]]) -> pd.DataFrame:\n",
    "    \"\"\"Convert benchmark data to a structured DataFrame.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for model_name, runs in benchmarks.items():\n",
    "        for run_idx, run_data in enumerate(runs):\n",
    "            # Extract run metadata\n",
    "            timestamp = datetime.fromtimestamp(run_data[\"benchmark_start_timestamp\"])\n",
    "            duration = (\n",
    "                run_data[\"benchmark_end_timestamp\"]\n",
    "                - run_data[\"benchmark_start_timestamp\"]\n",
    "            )\n",
    "\n",
    "            # Process each metric entry\n",
    "            for sample_idx, metric in enumerate(run_data[\"metrics\"]):\n",
    "                row = {\n",
    "                    \"model\": model_name,\n",
    "                    \"run_idx\": run_idx,\n",
    "                    \"sample_idx\": sample_idx,\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"duration\": duration,\n",
    "                    \"prediction_length\": run_data[\"prediction_length\"],\n",
    "                    \"context_length\": run_data[\"context_length\"],\n",
    "                    \"window\": run_data[\"window\"],\n",
    "                    **metric,  # Unpack all metric values\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Process data\n",
    "df = process_benchmark_data(all_benchmarks)\n",
    "\n",
    "print(f\"üìà Processed {len(df)} individual predictions across all models\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregate statistics per model\n",
    "metric_columns = [\n",
    "    \"MAE\",\n",
    "    \"RMSE\",\n",
    "    \"NRMSE\",\n",
    "    \"ND\",\n",
    "    \"MAPE\",\n",
    "    \"sMAPE\",\n",
    "    \"MASE\",\n",
    "    \"Directional Accuracy\",\n",
    "]\n",
    "\n",
    "summary_stats = df.groupby(\"model\")[metric_columns].agg(\n",
    "    [\"mean\", \"std\", \"median\", \"min\", \"max\"]\n",
    ")\n",
    "\n",
    "print(\"üìä Summary Statistics by Model\\n\")\n",
    "print(\"=\" * 100)\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 5. Model Comparison - Mean Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table with mean metrics\n",
    "comparison_df = df.groupby(\"model\")[metric_columns].mean().round(4)\n",
    "\n",
    "# Add ranking for each metric (lower is better except Directional Accuracy)\n",
    "for col in metric_columns:\n",
    "    if col == \"Directional Accuracy\":\n",
    "        comparison_df[f\"{col}_rank\"] = comparison_df[col].rank(ascending=False)\n",
    "    else:\n",
    "        comparison_df[f\"{col}_rank\"] = comparison_df[col].rank(ascending=True)\n",
    "\n",
    "# Calculate average rank\n",
    "rank_cols = [col for col in comparison_df.columns if col.endswith(\"_rank\")]\n",
    "comparison_df[\"avg_rank\"] = comparison_df[rank_cols].mean(axis=1)\n",
    "comparison_df = comparison_df.sort_values(\"avg_rank\")\n",
    "\n",
    "print(\"üèÜ Model Comparison - Mean Metrics\\n\")\n",
    "comparison_df[metric_columns + [\"avg_rank\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 6. Visualization - Error Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot key error metrics\n",
    "error_metrics = [\"MAE\", \"RMSE\", \"NRMSE\", \"ND\", \"MAPE\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(error_metrics):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Box plot\n",
    "    df.boxplot(column=metric, by=\"model\", ax=ax)\n",
    "    ax.set_title(f\"{metric} Distribution by Model\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Model\", fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Error Metrics Comparison Across Models\", fontsize=16, fontweight=\"bold\", y=1.00\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 7. Visualization - Percentage Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot percentage-based metrics\n",
    "percentage_metrics = [\"MAPE\", \"sMAPE\", \"MASE\", \"Directional Accuracy\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(percentage_metrics):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Box plot\n",
    "    df.boxplot(column=metric, by=\"model\", ax=ax)\n",
    "    ax.set_title(f\"{metric} Distribution by Model\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Model\", fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Percentage-Based Metrics Comparison\", fontsize=16, fontweight=\"bold\", y=1.00\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 8. Radar Chart - Multi-Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# Normalize metrics to 0-1 scale for comparison\n",
    "normalized_df = comparison_df[metric_columns].copy()\n",
    "\n",
    "for col in metric_columns:\n",
    "    if col == \"Directional Accuracy\":\n",
    "        # Higher is better - normalize to 0-1\n",
    "        normalized_df[col] = normalized_df[col] / 100.0\n",
    "    else:\n",
    "        # Lower is better - invert and normalize\n",
    "        max_val = normalized_df[col].max()\n",
    "        min_val = normalized_df[col].min()\n",
    "        if max_val != min_val:\n",
    "            normalized_df[col] = 1 - (normalized_df[col] - min_val) / (\n",
    "                max_val - min_val\n",
    "            )\n",
    "        else:\n",
    "            normalized_df[col] = 1.0\n",
    "\n",
    "# Create radar chart\n",
    "categories = metric_columns\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection=\"polar\"))\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(normalized_df)))\n",
    "\n",
    "for idx, (model, row) in enumerate(normalized_df.iterrows()):\n",
    "    values = row.values.tolist()\n",
    "    values += values[:1]\n",
    "\n",
    "    ax.plot(\n",
    "        angles, values, \"o-\", linewidth=2, label=model.split(\"/\")[-1], color=colors[idx]\n",
    "    )\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], size=9)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "plt.title(\n",
    "    \"Multi-Metric Model Comparison (Normalized)\", size=16, fontweight=\"bold\", pad=20\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: All metrics normalized to 0-1 scale where 1.0 = best performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 9. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def perform_pairwise_tests(df, metric, alpha=0.05):\n",
    "    \"\"\"Perform pairwise Mann-Whitney U tests between models.\"\"\"\n",
    "    models = df[\"model\"].unique()\n",
    "\n",
    "    results = pd.DataFrame(index=models, columns=models)\n",
    "\n",
    "    for i, model1 in enumerate(models):\n",
    "        for j, model2 in enumerate(models):\n",
    "            if i == j:\n",
    "                results.loc[model1, model2] = 1.0\n",
    "            else:\n",
    "                data1 = df[df[\"model\"] == model1][metric].dropna()\n",
    "                data2 = df[df[\"model\"] == model2][metric].dropna()\n",
    "\n",
    "                if len(data1) > 0 and len(data2) > 0:\n",
    "                    _, p_value = stats.mannwhitneyu(\n",
    "                        data1, data2, alternative=\"two-sided\"\n",
    "                    )\n",
    "                    results.loc[model1, model2] = p_value\n",
    "                else:\n",
    "                    results.loc[model1, model2] = np.nan\n",
    "\n",
    "    return results.astype(float)\n",
    "\n",
    "\n",
    "# Perform tests for key metrics\n",
    "key_metrics = [\"MAE\", \"RMSE\", \"Directional Accuracy\"]\n",
    "\n",
    "print(\"üî¨ Statistical Significance Tests (Mann-Whitney U)\\n\")\n",
    "print(\"P-values for pairwise comparisons (Œ± = 0.05)\\n\")\n",
    "\n",
    "for metric in key_metrics:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Metric: {metric}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    p_values = perform_pairwise_tests(df, metric)\n",
    "    print(p_values.round(4))\n",
    "    print(\"\\nSignificant differences (p < 0.05): ‚úì\")\n",
    "    print((p_values < 0.05).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 10. Performance vs Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by model and run\n",
    "perf_time_df = (\n",
    "    df.groupby([\"model\", \"run_idx\"])\n",
    "    .agg({\"MAE\": \"mean\", \"RMSE\": \"mean\", \"duration\": \"first\", \"sample_idx\": \"count\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "perf_time_df.rename(columns={\"sample_idx\": \"num_samples\"}, inplace=True)\n",
    "perf_time_df[\"samples_per_second\"] = (\n",
    "    perf_time_df[\"num_samples\"] / perf_time_df[\"duration\"]\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# MAE vs Duration\n",
    "for model in perf_time_df[\"model\"].unique():\n",
    "    model_data = perf_time_df[perf_time_df[\"model\"] == model]\n",
    "    ax1.scatter(\n",
    "        model_data[\"duration\"],\n",
    "        model_data[\"MAE\"],\n",
    "        label=model.split(\"/\")[-1],\n",
    "        s=100,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Execution Time (seconds)\", fontsize=12)\n",
    "ax1.set_ylabel(\"Mean Absolute Error\", fontsize=12)\n",
    "ax1.set_title(\"Performance vs Execution Time\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput comparison\n",
    "throughput_summary = (\n",
    "    perf_time_df.groupby(\"model\")[\"samples_per_second\"].mean().sort_values()\n",
    ")\n",
    "throughput_summary.plot(kind=\"barh\", ax=ax2, color=\"skyblue\", edgecolor=\"black\")\n",
    "ax2.set_xlabel(\"Samples per Second\", fontsize=12)\n",
    "ax2.set_title(\"Average Throughput by Model\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö° Throughput Summary:\")\n",
    "print(throughput_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 11. Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(data, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method.\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data.dropna()))\n",
    "    return z_scores > threshold\n",
    "\n",
    "\n",
    "# Analyze outliers for each model\n",
    "print(\"üîç Outlier Analysis (Z-score > 3)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for model in df[\"model\"].unique():\n",
    "    model_data = df[df[\"model\"] == model]\n",
    "\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for metric in [\"MAE\", \"RMSE\", \"MAPE\"]:\n",
    "        outliers = detect_outliers(model_data[metric])\n",
    "        n_outliers = outliers.sum()\n",
    "        pct_outliers = (n_outliers / len(model_data)) * 100\n",
    "\n",
    "        print(f\"  {metric}: {n_outliers} outliers ({pct_outliers:.2f}%)\")\n",
    "\n",
    "        outlier_summary.append(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"metric\": metric,\n",
    "                \"n_outliers\": n_outliers,\n",
    "                \"pct_outliers\": pct_outliers,\n",
    "            }\n",
    "        )\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "\n",
    "# Visualize outlier percentages\n",
    "pivot_outliers = outlier_df.pivot(\n",
    "    index=\"model\", columns=\"metric\", values=\"pct_outliers\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "pivot_outliers.plot(kind=\"bar\", width=0.8)\n",
    "plt.title(\"Percentage of Outliers by Model and Metric\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "plt.ylabel(\"Outlier Percentage (%)\", fontsize=12)\n",
    "plt.legend(title=\"Metric\", fontsize=10)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 12. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "print(\"üìã Model Recommendations\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Best overall performer\n",
    "best_model = comparison_df[\"avg_rank\"].idxmin()\n",
    "print(f\"\\nüèÜ BEST OVERALL MODEL: {best_model}\")\n",
    "print(f\"   Average Rank: {comparison_df.loc[best_model, 'avg_rank']:.2f}\")\n",
    "\n",
    "# Best for accuracy\n",
    "best_mae = comparison_df[\"MAE\"].idxmin()\n",
    "print(f\"\\nüéØ BEST FOR ACCURACY (Lowest MAE): {best_mae}\")\n",
    "print(f\"   MAE: {comparison_df.loc[best_mae, 'MAE']:.4f}\")\n",
    "\n",
    "# Best for directional accuracy\n",
    "best_dir_acc = comparison_df[\"Directional Accuracy\"].idxmax()\n",
    "print(f\"\\nüìà BEST FOR TREND PREDICTION: {best_dir_acc}\")\n",
    "print(\n",
    "    f\"   Directional Accuracy: {comparison_df.loc[best_dir_acc, 'Directional Accuracy']:.2f}%\"\n",
    ")\n",
    "\n",
    "# Fastest model\n",
    "fastest_model = throughput_summary.idxmax()\n",
    "print(f\"\\n‚ö° FASTEST MODEL: {fastest_model}\")\n",
    "print(f\"   Throughput: {throughput_summary[fastest_model]:.2f} samples/sec\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° CONCLUSION:\")\n",
    "print(\"\\nConsider your priorities when selecting a model:\")\n",
    "print(\"  ‚Ä¢ For highest accuracy: Choose the model with lowest MAE/RMSE\")\n",
    "print(\"  ‚Ä¢ For trend following: Choose the model with highest Directional Accuracy\")\n",
    "print(\"  ‚Ä¢ For speed: Choose the model with highest throughput\")\n",
    "print(\"  ‚Ä¢ For balanced performance: Choose the model with best average rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 13. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary report\n",
    "report_data = {\n",
    "    \"summary_statistics\": summary_stats,\n",
    "    \"model_comparison\": comparison_df,\n",
    "    \"throughput\": throughput_summary,\n",
    "    \"outlier_analysis\": outlier_df,\n",
    "}\n",
    "\n",
    "# Save to Excel with multiple sheets\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_file = f\"benchmark_analysis_report_{timestamp}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(report_file, engine=\"openpyxl\") as writer:\n",
    "    comparison_df[metric_columns + [\"avg_rank\"]].to_excel(\n",
    "        writer, sheet_name=\"Model_Comparison\"\n",
    "    )\n",
    "    throughput_summary.to_excel(writer, sheet_name=\"Throughput\")\n",
    "    outlier_df.to_excel(writer, sheet_name=\"Outliers\", index=False)\n",
    "    df.groupby(\"model\")[metric_columns].describe().to_excel(\n",
    "        writer, sheet_name=\"Detailed_Stats\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Report saved to: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusiontimeseries (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
