{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Load V2 Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmarkv2_files(benchmark_dir: Path) -> Dict[str, Dict]:\n",
    "    \"\"\"Load only the latest V2 benchmark JSON file per model.\"\"\"\n",
    "    benchmarks = {}\n",
    "    model_files = {}\n",
    "\n",
    "    # First pass: collect all V2 files per model\n",
    "    for json_file in benchmark_dir.glob(\"*benchmarkv2*.json\"):\n",
    "        with open(json_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            model_name = data[\"model\"]\n",
    "\n",
    "            # Extract timestamp from filename\n",
    "            filename = json_file.name\n",
    "            timestamp_str = filename.split(\"_\")[0] + \"_\" + filename.split(\"_\")[1]\n",
    "\n",
    "            if model_name not in model_files:\n",
    "                model_files[model_name] = []\n",
    "            model_files[model_name].append((timestamp_str, json_file, data))\n",
    "\n",
    "    # Second pass: keep only the latest file per model\n",
    "    for model_name, files in model_files.items():\n",
    "        files.sort(key=lambda x: x[0], reverse=True)\n",
    "        latest_file = files[0]\n",
    "\n",
    "        benchmarks[model_name] = latest_file[2]\n",
    "        print(f\"  üìÖ {model_name}: Using {latest_file[1].name}\")\n",
    "\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "# Load all V2 benchmarks\n",
    "benchmark_dir = Path(\"benchmarks\")\n",
    "all_benchmarks = load_benchmarkv2_files(benchmark_dir)\n",
    "\n",
    "print(f\"\\nüìä Loaded V2 benchmarks for {len(all_benchmarks)} models\")\n",
    "for model, data in all_benchmarks.items():\n",
    "    print(f\"  - {model}: {len(data['samples'])} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Denormalization and Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_prediction(\n",
    "    normalized_pred: np.ndarray, mean: float, std: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Denormalize predictions using stored mean and std.\"\"\"\n",
    "    return normalized_pred * std + mean\n",
    "\n",
    "\n",
    "def calculate_metrics(\n",
    "    pred: np.ndarray, target: np.ndarray, context: np.ndarray\n",
    ") -> Dict:\n",
    "    \"\"\"Calculate all forecasting metrics.\"\"\"\n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(pred - target))\n",
    "\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(np.mean((pred - target) ** 2))\n",
    "\n",
    "    # NRMSE\n",
    "    target_range = np.max(target) - np.min(target)\n",
    "    nrmse = rmse / target_range if target_range > 0 else float(\"inf\")\n",
    "\n",
    "    # ND (Normalized Deviation)\n",
    "    nd = (\n",
    "        np.sum(np.abs(pred - target)) / np.sum(np.abs(target))\n",
    "        if np.sum(np.abs(target)) > 0\n",
    "        else float(\"inf\")\n",
    "    )\n",
    "\n",
    "    # MAPE\n",
    "    epsilon = 1e-8\n",
    "    mape = 100.0 * np.mean(np.abs((pred - target) / (np.abs(target) + epsilon)))\n",
    "\n",
    "    # sMAPE\n",
    "    smape = 100.0 * np.mean(\n",
    "        2.0 * np.abs(pred - target) / (np.abs(pred) + np.abs(target) + epsilon)\n",
    "    )\n",
    "\n",
    "    # MASE\n",
    "    naive_forecast_error = np.mean(np.abs(np.diff(target)))\n",
    "    mase = mae / naive_forecast_error if naive_forecast_error > 0 else float(\"inf\")\n",
    "\n",
    "    # Directional Accuracy\n",
    "    pred_direction = np.sign(np.diff(pred))\n",
    "    target_direction = np.sign(np.diff(target))\n",
    "    directional_acc = 100.0 * np.mean(pred_direction == target_direction)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"NRMSE\": nrmse,\n",
    "        \"ND\": nd,\n",
    "        \"MAPE\": mape,\n",
    "        \"sMAPE\": smape,\n",
    "        \"MASE\": mase,\n",
    "        \"Directional Accuracy\": directional_acc,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Denormalization and metric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 4. Process All Samples and Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_benchmarkv2_data(benchmarks: Dict[str, Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Process V2 benchmark data and calculate metrics for each sample.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for model_name, benchmark_data in benchmarks.items():\n",
    "        for sample in benchmark_data[\"samples\"]:\n",
    "            sample_id = sample[\"sample_id\"]\n",
    "            context = np.array(sample[\"context\"])\n",
    "            target = np.array(sample[\"target\"])\n",
    "            pred_quantiles_norm = np.array(\n",
    "                sample[\"prediction_quantiles\"]\n",
    "            )  # [n_quantiles, pred_len]\n",
    "            mean = sample[\"normalization_mean\"]\n",
    "            std = sample[\"normalization_std\"]\n",
    "\n",
    "            # Denormalize predictions\n",
    "            pred_quantiles = denormalize_prediction(pred_quantiles_norm, mean, std)\n",
    "\n",
    "            # Extract median prediction (middle quantile)\n",
    "            n_quantiles = pred_quantiles.shape[0]\n",
    "            median_pred = pred_quantiles[n_quantiles // 2, :]\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = calculate_metrics(median_pred, target, context)\n",
    "\n",
    "            # Store everything\n",
    "            row = {\n",
    "                \"model\": model_name,\n",
    "                \"sample_id\": sample_id,\n",
    "                \"context\": context,\n",
    "                \"target\": target,\n",
    "                \"prediction\": median_pred,\n",
    "                \"pred_quantiles\": pred_quantiles,\n",
    "                \"mean\": mean,\n",
    "                \"std\": std,\n",
    "                **metrics,\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Process all data\n",
    "df = process_benchmarkv2_data(all_benchmarks)\n",
    "\n",
    "print(f\"üìà Processed {len(df)} samples across all models\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(\n",
    "    f\"\\nColumns: {[col for col in df.columns if col not in ['context', 'target', 'prediction', 'pred_quantiles']]}\"\n",
    ")\n",
    "df[\n",
    "    [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if col not in [\"context\", \"target\", \"prediction\", \"pred_quantiles\"]\n",
    "    ]\n",
    "].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "metric_columns = [\n",
    "    \"MAE\",\n",
    "    \"RMSE\",\n",
    "    \"NRMSE\",\n",
    "    \"ND\",\n",
    "    \"MAPE\",\n",
    "    \"sMAPE\",\n",
    "    \"MASE\",\n",
    "    \"Directional Accuracy\",\n",
    "]\n",
    "\n",
    "summary = df.groupby(\"model\")[metric_columns].agg(\n",
    "    [\"mean\", \"std\", \"median\", \"min\", \"max\"]\n",
    ")\n",
    "\n",
    "print(\"üìä Summary Statistics by Model (V2 - Properly Denormalized)\\n\")\n",
    "print(\"=\" * 120)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 6. Plotting Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_prediction(row, show_quantiles=True):\n",
    "    \"\"\"Plot a single sample's context, target, and prediction.\"\"\"\n",
    "    context = row[\"context\"]\n",
    "    target = row[\"target\"]\n",
    "    prediction = row[\"prediction\"]\n",
    "    pred_quantiles = row[\"pred_quantiles\"]\n",
    "\n",
    "    context_len = len(context)\n",
    "    pred_len = len(prediction)\n",
    "\n",
    "    context_x = range(context_len)\n",
    "    future_x = range(context_len, context_len + pred_len)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot context\n",
    "    plt.plot(context_x, context, label=\"Context\", color=\"#4a90d9\", linewidth=2)\n",
    "\n",
    "    # Plot target\n",
    "    plt.plot(\n",
    "        future_x,\n",
    "        target,\n",
    "        label=\"Ground Truth\",\n",
    "        color=\"#2ecc71\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Plot median prediction\n",
    "    plt.plot(\n",
    "        future_x,\n",
    "        prediction,\n",
    "        label=\"Prediction (Median)\",\n",
    "        color=\"#e74c3c\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "\n",
    "    # Plot quantiles if requested\n",
    "    if show_quantiles and pred_quantiles is not None:\n",
    "        n_quantiles = pred_quantiles.shape[0]\n",
    "        lower_q = pred_quantiles[0, :]\n",
    "        upper_q = pred_quantiles[-1, :]\n",
    "\n",
    "        plt.fill_between(\n",
    "            future_x,\n",
    "            lower_q,\n",
    "            upper_q,\n",
    "            color=\"#e74c3c\",\n",
    "            alpha=0.2,\n",
    "            label=f\"Prediction Range ({n_quantiles} quantiles)\",\n",
    "        )\n",
    "\n",
    "    # Add metrics as text\n",
    "    metrics_text = (\n",
    "        f\"MAE: {row['MAE']:.4f}  |  \"\n",
    "        f\"RMSE: {row['RMSE']:.4f}  |  \"\n",
    "        f\"MAPE: {row['MAPE']:.2f}%  |  \"\n",
    "        f\"sMAPE: {row['sMAPE']:.2f}%  |  \"\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        f\"Model: {row['model']} | Sample ID: {row['sample_id']}\\n{metrics_text}\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.xlabel(\"Time Step\", fontsize=11)\n",
    "    plt.ylabel(\"Value\", fontsize=11)\n",
    "    plt.legend(loc=\"best\", fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Plotting functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 7. Identify Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find worst predictions by MAE for each model\n",
    "print(\"üî¥ Worst Predictions by MAE (Top 3 per model)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model in df[\"model\"].unique():\n",
    "    model_data = df[df[\"model\"] == model]\n",
    "    worst_samples = model_data.nlargest(3, \"sMAPE\")\n",
    "\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for idx, row in worst_samples.iterrows():\n",
    "        print(\n",
    "            f\"  Sample {row['sample_id']}: MAE={row['MAE']:.4f}, RMSE={row['RMSE']:.4f}, MAPE={row['MAPE']:.2f}% , sMAPE={row['sMAPE']:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 8. Plot Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the worst prediction for each model\n",
    "print(\"Plotting worst prediction for each model...\\n\")\n",
    "\n",
    "for model in df[\"model\"].unique():\n",
    "    model_data = df[df[\"model\"] == model]\n",
    "    worst_sample = model_data.nlargest(1, \"sMAPE\").iloc[0]\n",
    "\n",
    "    plot_sample_prediction(worst_sample, show_quantiles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 9. Identify Best Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best predictions by MAE for each model\n",
    "print(\"üü¢ Best Predictions by MAE (Top 3 per model)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model in df[\"model\"].unique():\n",
    "    model_data = df[df[\"model\"] == model]\n",
    "    best_samples = model_data.nsmallest(3, \"MAPE\")\n",
    "\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for idx, row in best_samples.iterrows():\n",
    "        print(\n",
    "            f\"  Sample {row['sample_id']}: MAE={row['MAE']:.4f}, RMSE={row['RMSE']:.4f}, MAPE={row['MAPE']:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 10. Compare Models on Same Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models on a specific sample (e.g., sample_id=0)\n",
    "sample_id_to_compare = 1560\n",
    "\n",
    "print(f\"Comparing all models on Sample ID: {sample_id_to_compare}\\n\")\n",
    "\n",
    "for model in df[\"model\"].unique():\n",
    "    model_sample = df[\n",
    "        (df[\"model\"] == model) & (df[\"sample_id\"] == sample_id_to_compare)\n",
    "    ]\n",
    "\n",
    "    if not model_sample.empty:\n",
    "        plot_sample_prediction(model_sample.iloc[0], show_quantiles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 11. Interactive Sample Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explore any sample by model and sample_id\n",
    "def explore_sample(model_name: str, sample_id: int, show_quantiles: bool = True):\n",
    "    \"\"\"Plot a specific sample for a specific model.\"\"\"\n",
    "    sample_data = df[(df[\"model\"] == model_name) & (df[\"sample_id\"] == sample_id)]\n",
    "\n",
    "    if sample_data.empty:\n",
    "        print(f\"‚ùå No data found for model '{model_name}' and sample_id {sample_id}\")\n",
    "        return\n",
    "\n",
    "    plot_sample_prediction(sample_data.iloc[0], show_quantiles=show_quantiles)\n",
    "\n",
    "\n",
    "# Example usage - change these values to explore different samples\n",
    "print(\"\\nüí° Use explore_sample(model_name, sample_id) to view specific predictions\")\n",
    "print(f\"\\nAvailable models: {df['model'].unique().tolist()}\")\n",
    "print(f\"Sample ID range: 0 to {df['sample_id'].max()}\")\n",
    "\n",
    "# Uncomment and modify to explore:\n",
    "idx = 609\n",
    "explore_sample(\"amazon/chronos-2\", idx)\n",
    "explore_sample(\"NX-AI/TiRex\", idx)\n",
    "explore_sample(\"google/timesfm-2.5-200m-pytorch\", idx)\n",
    "explore_sample(\"amazon/chronos-bolt-tiny\", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 12. Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "error_metrics = [\"MAE\", \"RMSE\", \"MAPE\", \"Directional Accuracy\"]\n",
    "\n",
    "for idx, metric in enumerate(error_metrics):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for model in df[\"model\"].unique():\n",
    "        model_data = df[df[\"model\"] == model][metric].dropna()\n",
    "        ax.hist(\n",
    "            model_data,\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "            label=model.split(\"/\")[-1],\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"{metric} Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(metric, fontsize=12)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Error Distribution Comparison (V2 Benchmarks)\", fontsize=16, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 13. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "summary_file = f\"benchmarkv2_summary_{timestamp}.csv\"\n",
    "\n",
    "# Export metrics only (without timeseries data)\n",
    "export_columns = [\"model\", \"sample_id\", \"mean\", \"std\"] + metric_columns\n",
    "\n",
    "df[export_columns].to_csv(summary_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Summary exported to: {summary_file}\")\n",
    "print(\"\\nüìä Final Model Rankings (by mean MAE):\")\n",
    "print(df.groupby(\"model\")[\"MAE\"].mean().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusiontimeseries (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
