{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chronos-2 Few-Shot In-Context Learning Benchmark\n",
    "\n",
    "This notebook benchmarks Chronos-2's few-shot in-context learning (ICL) capabilities for flux time-series prediction.\n",
    "\n",
    "**Approach:**\n",
    "- Provide k example traces (context + target pairs) before the query\n",
    "- Test k=1, 3, 5, 10\n",
    "- Random example selection with fixed seed\n",
    "- Compare to zero-shot baseline\n",
    "\n",
    "**Format:**\n",
    "```\n",
    "[ex1_context(80), ex1_target(64), ex2_context(80), ex2_target(64), ..., query_context(80)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from chronos import Chronos2Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fusiontimeseries.benchmarking.zero_shot.benchmark_utils import (\n",
    "    BenchmarkDataProvider,\n",
    "    IN_DISTRIBUTION_ITERATIONS,\n",
    "    OUT_OF_DISTRIBUTION_ITERATIONS,\n",
    "    Utils,\n",
    "    rmse_with_standard_error,\n",
    ")\n",
    "from fusiontimeseries.benchmarking.few_shot.few_shot_utils import (\n",
    "    FewShotConfig,\n",
    "    create_example_pool,\n",
    "    select_examples_random,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "K_SHOT = 10  # Change to test different k values: 1, 3, 5, 10\n",
    "\n",
    "config = FewShotConfig(\n",
    "    model_slug=\"amazon/chronos-2\",\n",
    "    model_prediction_length=64,\n",
    "    start_context_length=80,\n",
    "    relevant_prediction_tail=80,\n",
    "    k_shot=K_SHOT,\n",
    "    random_seed=42,\n",
    ")\n",
    "print(f\"Configuration: k={config.k_shot}, seed={config.random_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "provider = BenchmarkDataProvider()\n",
    "pipeline: Chronos2Pipeline = Chronos2Pipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path=config.model_slug,\n",
    "    device_map=config.device,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "print(f\"Model loaded on device: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example pool (exclude test IDs)\n",
    "test_ids = {8, 115, 131, 148, 235, 262}\n",
    "example_pool = create_example_pool(exclude_ids=test_ids)\n",
    "\n",
    "# Verify no test set leakage\n",
    "pool_ids = {ex.trace_id for ex in example_pool}\n",
    "assert not (pool_ids & test_ids), \"ERROR: Test IDs found in example pool!\"\n",
    "print(f\"✓ Example pool created: {len(example_pool)} traces\")\n",
    "print(\"✓ No test set leakage verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_autoregressive_forecast(\n",
    "    trace: np.ndarray,\n",
    "    examples: list,\n",
    "    config: FewShotConfig,\n",
    "    pipeline: Chronos2Pipeline,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Autoregressively forecast using few-shot examples.\n",
    "\n",
    "    Strategy:\n",
    "    1. Normalize each example independently\n",
    "    2. Normalize query independently\n",
    "    3. Format as [ex1_ctx, ex1_tgt, ex2_ctx, ex2_tgt, ..., query_ctx]\n",
    "    4. Prepend examples to context at each autoregressive step\n",
    "    5. Denormalize using query scaler\n",
    "\n",
    "    Args:\n",
    "        trace: Ground truth trace [266]\n",
    "        examples: List of k FewShotExample objects\n",
    "        config: Benchmark configuration\n",
    "        pipeline: Chronos2 model pipeline\n",
    "\n",
    "    Returns:\n",
    "        Denormalized forecast [266]\n",
    "    \"\"\"\n",
    "    trace_length = trace.shape[0]\n",
    "\n",
    "    # Normalize examples (each independently)\n",
    "    normalized_examples = []\n",
    "    for ex in examples:\n",
    "        ex_scaler = StandardScaler()\n",
    "        normed_ctx = ex_scaler.fit_transform(ex.context_array.reshape(-1, 1)).squeeze()\n",
    "        normed_tgt = ex_scaler.transform(ex.target_array.reshape(-1, 1)).squeeze()\n",
    "        normalized_examples.append(\n",
    "            {\n",
    "                \"context\": normed_ctx,\n",
    "                \"target\": normed_tgt,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Normalize query context\n",
    "    query_scaler = StandardScaler()\n",
    "    initial_query_context = trace[: config.start_context_length]\n",
    "    normed_query_ctx = query_scaler.fit_transform(\n",
    "        initial_query_context.reshape(-1, 1)\n",
    "    ).squeeze()\n",
    "\n",
    "    # Start with initial ICL context\n",
    "    current_query = normed_query_ctx.copy()\n",
    "    predictions = [initial_query_context]  # Store denormalized predictions\n",
    "\n",
    "    # Autoregressive prediction\n",
    "    while len(np.concatenate(predictions)) < trace_length:\n",
    "        # Format ICL context with examples\n",
    "        icl_segments = []\n",
    "        for ex_norm in normalized_examples:\n",
    "            icl_segments.append(ex_norm[\"context\"])\n",
    "            icl_segments.append(ex_norm[\"target\"])\n",
    "        icl_segments.append(current_query)\n",
    "\n",
    "        icl_context = np.concatenate(icl_segments)\n",
    "\n",
    "        # Convert to tensor: [1, 1, context_length]\n",
    "        ctx_tensor = (\n",
    "            torch.tensor(icl_context, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        # Predict next 64 steps\n",
    "        forecast: list[torch.Tensor] = pipeline.predict(\n",
    "            inputs=ctx_tensor,\n",
    "            prediction_length=config.model_prediction_length,\n",
    "        )\n",
    "\n",
    "        # Extract median forecast\n",
    "        quantiles: torch.Tensor = forecast[0].permute(\n",
    "            0, 2, 1\n",
    "        )  # [1, pred_len, n_quantiles]\n",
    "        median_forecast = Utils.median_forecast(quantiles).squeeze().cpu().numpy()\n",
    "\n",
    "        # Denormalize prediction\n",
    "        denormed_pred = query_scaler.inverse_transform(\n",
    "            median_forecast.reshape(-1, 1)\n",
    "        ).squeeze()\n",
    "\n",
    "        predictions.append(denormed_pred)\n",
    "\n",
    "        # Update current query: append normalized prediction\n",
    "        # Re-normalize the extended context\n",
    "        extended_denormed = np.concatenate(predictions)\n",
    "        current_query = query_scaler.transform(\n",
    "            extended_denormed.reshape(-1, 1)\n",
    "        ).squeeze()\n",
    "\n",
    "    # Concatenate and trim to trace length\n",
    "    full_forecast = np.concatenate(predictions)[:trace_length]\n",
    "    return full_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run In-Distribution Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_forecast = {\n",
    "    \"in_distribution\": {},\n",
    "    \"out_of_distribution\": {},\n",
    "}\n",
    "\n",
    "for trace_id in IN_DISTRIBUTION_ITERATIONS:\n",
    "    print(f\"Processing ID trace: {trace_id}\")\n",
    "\n",
    "    trace = provider.get_id(trace_id).numpy()\n",
    "\n",
    "    # Select k examples for this trace\n",
    "    examples = select_examples_random(\n",
    "        example_pool,\n",
    "        k=config.k_shot,\n",
    "        seed=config.random_seed,\n",
    "    )\n",
    "    print(f\"  Selected examples: {[ex.trace_id for ex in examples]}\")\n",
    "\n",
    "    # Run few-shot prediction\n",
    "    forecast = fewshot_autoregressive_forecast(trace, examples, config, pipeline)\n",
    "\n",
    "    trace_forecast[\"in_distribution\"][trace_id] = (trace, forecast)\n",
    "    print(f\"  ✓ Forecast shape: {forecast.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Out-of-Distribution Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace_id in OUT_OF_DISTRIBUTION_ITERATIONS:\n",
    "    print(f\"Processing OOD trace: {trace_id}\")\n",
    "\n",
    "    trace = provider.get_ood(trace_id).numpy()\n",
    "\n",
    "    # Select k examples for this trace\n",
    "    examples = select_examples_random(\n",
    "        example_pool,\n",
    "        k=config.k_shot,\n",
    "        seed=config.random_seed,\n",
    "    )\n",
    "    print(f\"  Selected examples: {[ex.trace_id for ex in examples]}\")\n",
    "\n",
    "    # Run few-shot prediction\n",
    "    forecast = fewshot_autoregressive_forecast(trace, examples, config, pipeline)\n",
    "\n",
    "    trace_forecast[\"out_of_distribution\"][trace_id] = (trace, forecast)\n",
    "    print(f\"  ✓ Forecast shape: {forecast.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_means = {\n",
    "    \"in_distribution\": {\n",
    "        \"ground_truth\": [],\n",
    "        \"forecast\": [],\n",
    "    },\n",
    "    \"out_of_distribution\": {\n",
    "        \"ground_truth\": [],\n",
    "        \"forecast\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Compute means for last 80 timesteps (evaluation window)\n",
    "for trace_id, (y_true, y_pred) in trace_forecast[\"in_distribution\"].items():\n",
    "    trace_mean = np.mean(y_true[-config.relevant_prediction_tail :])\n",
    "    forecast_mean = np.mean(y_pred[-config.relevant_prediction_tail :])\n",
    "    trace_means[\"in_distribution\"][\"ground_truth\"].append(trace_mean)\n",
    "    trace_means[\"in_distribution\"][\"forecast\"].append(forecast_mean)\n",
    "\n",
    "for trace_id, (y_true, y_pred) in trace_forecast[\"out_of_distribution\"].items():\n",
    "    trace_mean = np.mean(y_true[-config.relevant_prediction_tail :])\n",
    "    forecast_mean = np.mean(y_pred[-config.relevant_prediction_tail :])\n",
    "    trace_means[\"out_of_distribution\"][\"ground_truth\"].append(trace_mean)\n",
    "    trace_means[\"out_of_distribution\"][\"forecast\"].append(forecast_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE with standard error\n",
    "rmse_id, se_rmse_id = rmse_with_standard_error(\n",
    "    np.array(trace_means[\"in_distribution\"][\"ground_truth\"]),\n",
    "    np.array(trace_means[\"in_distribution\"][\"forecast\"]),\n",
    ")\n",
    "\n",
    "rmse_ood, se_rmse_ood = rmse_with_standard_error(\n",
    "    np.array(trace_means[\"out_of_distribution\"][\"ground_truth\"]),\n",
    "    np.array(trace_means[\"out_of_distribution\"][\"forecast\"]),\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"CHRONOS-2 FEW-SHOT (k={config.k_shot}) RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ID RMSE:  {rmse_id:.4f} ± {se_rmse_id:.4f}\")\n",
    "print(f\"OOD RMSE: {rmse_ood:.4f} ± {se_rmse_ood:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nZero-shot baseline (for comparison):\")\n",
    "print(\"ID RMSE:  84.86 ± 14.18\")\n",
    "print(\"OOD RMSE: 60.78 ± 12.75\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare results\n",
    "model_name_clean = config.model_slug.replace(\"/\", \"_\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "results = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"config\": config.model_dump(),\n",
    "    \"in_distribution\": {\n",
    "        \"rmse\": float(rmse_id),\n",
    "        \"se_rmse\": float(se_rmse_id),\n",
    "        \"n_samples\": len(trace_means[\"in_distribution\"][\"ground_truth\"]),\n",
    "    },\n",
    "    \"out_of_distribution\": {\n",
    "        \"rmse\": float(rmse_ood),\n",
    "        \"se_rmse\": float(se_rmse_ood),\n",
    "        \"n_samples\": len(trace_means[\"out_of_distribution\"][\"ground_truth\"]),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save results to JSON (project root / results / few_shot)\n",
    "data_dir = Path(\".\").resolve().parent.parent.parent.parent / \"results\" / \"few_shot\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_file = (\n",
    "    data_dir / f\"{timestamp}_{model_name_clean}_k{config.k_shot}_fewshot_results.json\"\n",
    ")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create plots directory\n",
    "plots_dir = data_dir / \"plots\" / f\"{timestamp}_{model_name_clean}_k{config.k_shot}\"\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot in-distribution traces\n",
    "for trace_id, (y_true, y_pred) in trace_forecast[\"in_distribution\"].items():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true, label=\"Ground Truth\", linewidth=2, alpha=0.7)\n",
    "    plt.plot(\n",
    "        y_pred,\n",
    "        label=f\"Few-Shot Forecast (k={config.k_shot})\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=config.start_context_length,\n",
    "        color=\"red\",\n",
    "        linestyle=\":\",\n",
    "        label=\"Forecast Start\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"Flux Value\")\n",
    "    plt.title(f\"{config.model_slug} Few-Shot (k={config.k_shot}) - ID: {trace_id}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_file = plots_dir / f\"id_{trace_id}.png\"\n",
    "    plt.savefig(plot_file, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Plot out-of-distribution traces\n",
    "for trace_id, (y_true, y_pred) in trace_forecast[\"out_of_distribution\"].items():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true, label=\"Ground Truth\", linewidth=2, alpha=0.7)\n",
    "    plt.plot(\n",
    "        y_pred,\n",
    "        label=f\"Few-Shot Forecast (k={config.k_shot})\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=config.start_context_length,\n",
    "        color=\"red\",\n",
    "        linestyle=\":\",\n",
    "        label=\"Forecast Start\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"Flux Value\")\n",
    "    plt.title(f\"{config.model_slug} Few-Shot (k={config.k_shot}) - OOD: {trace_id}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_file = plots_dir / f\"ood_{trace_id}.png\"\n",
    "    plt.savefig(plot_file, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Plots saved to: {plots_dir}\")\n",
    "print(\n",
    "    f\"Total plots created: {len(trace_forecast['in_distribution']) + len(trace_forecast['out_of_distribution'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook benchmarked Chronos-2's few-shot in-context learning performance.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Test different k values (change `K_SHOT` at the top and rerun)\n",
    "2. Compare results across k=1, 3, 5, 10\n",
    "3. Analyze which k value provides best performance\n",
    "4. Compare to zero-shot baseline to quantify ICL benefit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusiontimeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
