{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Training and Test Data\n",
    "\n",
    "Augmented time series sub-sampled to every third timestep\n",
    "The tail of each time-series series[:-prediction_length] is stored in validation set for validating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.common import space\n",
    "\n",
    "chronos2_hyperparameters = {\n",
    "    # https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-model-zoo.html#pretrained-models\n",
    "    \"Chronos2\": {\n",
    "        \"fine_tune\": True,\n",
    "        \"fine_tune_mode\": \"lora\",\n",
    "        \"fine_tune_lr\": space.Real(1e-5, 1e-3, log=True),\n",
    "        \"fine_tune_steps\": 3000,\n",
    "        \"fine_tune_batch_size\": space.Categorical(32, 64, 128),\n",
    "        \"cross_learning\": space.Bool(),\n",
    "        \"fine_tune_lora_config\": {\"r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.1},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import cache\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fusiontimeseries.finetuning.preprocessing.utils import get_valid_flux_traces\n",
    "\n",
    "\n",
    "@cache\n",
    "def create_training_flux_dataframe() -> pd.DataFrame:\n",
    "    training_flux_traces: dict[int, np.ndarray] = get_valid_flux_traces(\n",
    "        full_subsampling=True\n",
    "    )\n",
    "\n",
    "    records = []\n",
    "    for item_id, flux_trace in training_flux_traces.items():\n",
    "        for t in range(flux_trace.shape[0]):\n",
    "            records.append(\n",
    "                {\n",
    "                    \"item_id\": item_id,\n",
    "                    \"timestamp\": pd.to_datetime(datetime(2000, 1, 1))\n",
    "                    + pd.to_timedelta(t, unit=\"ms\"),\n",
    "                    \"target\": flux_trace[t],\n",
    "                }\n",
    "            )\n",
    "    training_flux_df = pd.DataFrame(records)\n",
    "    return training_flux_df\n",
    "\n",
    "\n",
    "training_flux_df = create_training_flux_dataframe()\n",
    "training_flux_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame\n",
    "\n",
    "training_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    training_flux_df,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    ")\n",
    "training_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=64, target=\"target\", eval_metric=\"RMSE\", verbosity=4\n",
    ")\n",
    "predictor.fit(\n",
    "    train_data=training_data,  # your training TimeSeriesDataFrame\n",
    "    hyperparameters={**chronos2_hyperparameters},\n",
    "    time_limit=3600,\n",
    "    enable_ensemble=False,\n",
    "    verbosity=4,\n",
    "    hyperparameter_tune_kwargs={\n",
    "        \"num_trials\": 8,\n",
    "        \"searcher\": \"bayes\",\n",
    "        \"scheduler\": \"local\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "\n",
    "raw_predictor_path = input(\"Enter path to save the predictor: \")\n",
    "predictor = TimeSeriesPredictor.load(raw_predictor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete evaluation with automatic saving of all results\n",
    "from fusiontimeseries.finetuning.evaluation_utils import (\n",
    "    FinetuningConfig,\n",
    "    run_complete_evaluation,\n",
    ")\n",
    "\n",
    "# Create configuration object\n",
    "finetuning_config = FinetuningConfig(\n",
    "    model_name=\"Chronos2\\\\T2\",  # TODO add manually\n",
    "    prediction_length=predictor.prediction_length,\n",
    "    target=\"target\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    hyperparameters={\n",
    "        \"fine_tune\": True,\n",
    "        \"fine_tune_mode\": \"lora\",\n",
    "        \"fine_tune_lr\": 0.00048812550121497074,\n",
    "        \"fine_tune_steps\": 3000,\n",
    "        \"fine_tune_batch_size\": 64,\n",
    "        \"cross_learning\": 0,\n",
    "        \"fine_tune_lora_config\": {\"r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.1},\n",
    "    },  # TODO add manually\n",
    "    time_limit=3600,\n",
    "    start_context_length=80,\n",
    "    relevant_tail_length=80,\n",
    ")\n",
    "\n",
    "# Run complete evaluation (forecasts, evaluates, plots, and saves everything)\n",
    "results, json_path, plots_dir = run_complete_evaluation(\n",
    "    predictor=predictor,\n",
    "    config=finetuning_config,\n",
    "    training_data_size=training_data.num_items,\n",
    "    predictor_path=predictor.path if hasattr(predictor, \"path\") else None,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… All results saved!\")\n",
    "print(f\"ðŸ“Š Plots directory: {plots_dir}\")\n",
    "print(f\"ðŸ“„ JSON results: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusiontimeseries (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
