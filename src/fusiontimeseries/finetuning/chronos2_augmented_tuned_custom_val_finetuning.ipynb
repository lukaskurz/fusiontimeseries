{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Seemed like the augmented data approach had too little complexity or fine tune steps to accuractely capture all patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.finetuning.preprocessing.utils import (\n",
    "    create_train_and_test_flux_ts_dataframes,\n",
    ")\n",
    "\n",
    "train_flux_df, val_flux_df = create_train_and_test_flux_ts_dataframes(\n",
    "    n_discretation_quantiles=5, test_set_size=0.1\n",
    ")\n",
    "print(f\"Training set size: {len(train_flux_df)}\")\n",
    "print(f\"Validation set size: {len(val_flux_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.common import space\n",
    "\n",
    "chronos2_hyperparameters = {\n",
    "    # https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-model-zoo.html#pretrained-models\n",
    "    \"Chronos2\": {\n",
    "        \"fine_tune\": True,\n",
    "        \"fine_tune_mode\": \"lora\",\n",
    "        \"fine_tune_lr\": space.Real(1e-5, 1e-3, log=True),\n",
    "        \"fine_tune_steps\": 4000,  # increased from 3000 to 4000\n",
    "        \"fine_tune_batch_size\": space.Categorical(32, 64, 128),\n",
    "        \"cross_learning\": space.Bool(),\n",
    "        \"fine_tune_lora_config\": {\n",
    "            \"r\": 32,  # increased from 16 to 32\n",
    "            \"lora_alpha\": 64,  # set to 2 times r\n",
    "            \"lora_dropout\": 0.1,\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame\n",
    "\n",
    "training_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    train_flux_df,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    ")\n",
    "validation_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    val_flux_df,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    ")\n",
    "print(f\"Training set size: {training_data.size}\")\n",
    "print(f\"Validation set size: {validation_data.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=64, target=\"target\", eval_metric=\"RMSE\", verbosity=4\n",
    ")\n",
    "predictor.fit(\n",
    "    train_data=training_data,\n",
    "    tuning_data=validation_data,  # add custom validation data\n",
    "    hyperparameters={**chronos2_hyperparameters},\n",
    "    time_limit=3600,\n",
    "    enable_ensemble=False,\n",
    "    verbosity=4,\n",
    "    hyperparameter_tune_kwargs={\n",
    "        \"num_trials\": 6,\n",
    "        \"searcher\": \"bayes\",\n",
    "        \"scheduler\": \"local\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "\n",
    "raw_predictor_path = input(\"Enter path to save the predictor: \")\n",
    "predictor = TimeSeriesPredictor.load(raw_predictor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete evaluation with automatic saving of all results\n",
    "from fusiontimeseries.finetuning.evaluation_utils import (\n",
    "    FinetuningConfig,\n",
    "    run_complete_evaluation,\n",
    ")\n",
    "\n",
    "# Create configuration object\n",
    "finetuning_config = FinetuningConfig(\n",
    "    model_name=\"Chronos2\\\\T2\",  # TODO add manually\n",
    "    prediction_length=predictor.prediction_length,\n",
    "    target=\"target\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    hyperparameters={\n",
    "        \"fine_tune\": True,\n",
    "        \"fine_tune_mode\": \"lora\",\n",
    "        \"fine_tune_lr\": 0.00048812550121497074,\n",
    "        \"fine_tune_steps\": 4000,\n",
    "        \"fine_tune_batch_size\": 64,\n",
    "        \"cross_learning\": 0,\n",
    "        \"fine_tune_lora_config\": {\"r\": 32, \"lora_alpha\": 64, \"lora_dropout\": 0.1},\n",
    "    },  # TODO add manually\n",
    "    time_limit=3600,\n",
    "    start_context_length=80,\n",
    "    relevant_tail_length=80,\n",
    ")\n",
    "\n",
    "# Run complete evaluation (forecasts, evaluates, plots, and saves everything)\n",
    "results, json_path, plots_dir = run_complete_evaluation(\n",
    "    predictor=predictor,\n",
    "    config=finetuning_config,\n",
    "    training_data_size=training_data.num_items,\n",
    "    predictor_path=predictor.path if hasattr(predictor, \"path\") else None,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… All results saved!\")\n",
    "print(f\"ðŸ“Š Plots directory: {plots_dir}\")\n",
    "print(f\"ðŸ“„ JSON results: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "next experiments include setting prediction length to 80 to have validation window on tuning set for validation error to be an exact RMSE measure for the evaluation constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusiontimeseries (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
