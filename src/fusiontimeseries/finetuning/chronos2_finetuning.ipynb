{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# always run\n",
    "\n",
    "chronos2_hyperparameters = {\n",
    "    # https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-model-zoo.html#pretrained-models\n",
    "    \"Chronos2\": {\n",
    "        \"fine_tune\": True,\n",
    "        \"fine_tune_mode\": \"lora\",\n",
    "        \"fine_tune_lr\": 5e-5,\n",
    "        \"fine_tune_steps\": 3000,\n",
    "        \"fine_tune_batch_size\": 64,\n",
    "        \"cross_learning\": False,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import cache\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fusiontimeseries.finetuning.preprocessing.utils import get_valid_flux_traces\n",
    "\n",
    "\n",
    "@cache\n",
    "def create_training_flux_dataframe() -> pd.DataFrame:\n",
    "    training_flux_traces: dict[int, np.ndarray] = get_valid_flux_traces()\n",
    "\n",
    "    records = []\n",
    "    for item_id, flux_trace in training_flux_traces.items():\n",
    "        for t in range(flux_trace.shape[0]):\n",
    "            records.append(\n",
    "                {\n",
    "                    \"item_id\": item_id,\n",
    "                    \"timestamp\": pd.to_datetime(datetime(2000, 1, 1))\n",
    "                    + pd.to_timedelta(t, unit=\"ms\"),\n",
    "                    \"target\": flux_trace[t],\n",
    "                }\n",
    "            )\n",
    "    training_flux_df = pd.DataFrame(records)\n",
    "    return training_flux_df\n",
    "\n",
    "\n",
    "training_flux_df = create_training_flux_dataframe()\n",
    "training_flux_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame\n",
    "\n",
    "training_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    training_flux_df,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    ")\n",
    "training_data.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "The training data consists of 250 timeseries with 266 timestamps in form of an 1-dimensional np array. The goal is to autoregressively forecast on a held out test set from timestep 80 until timestep 266 with 6 in-distribution and 5 out-of-distribution timeseries with the same length. Informational only: Measured is the RMSE with standard error of the means on both in-distribution and out-of-distribution prediction tails (last 80 timesteps)\n",
    "\n",
    "- Chronos-2â€™s underlying training infrastructure handles sliding window creation from training data automatically.\n",
    "- Chronos series are transformer models -> trained to use long contexts.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=64,  # forecasting horizon (266-80)\n",
    "    target=\"target\",\n",
    "    eval_metric=\"RMSE\",\n",
    ")\n",
    "predictor.fit(\n",
    "    train_data=training_data,  # your training TimeSeriesDataFrame\n",
    "    hyperparameters={**chronos2_hyperparameters},\n",
    "    time_limit=3600,\n",
    "    enable_ensemble=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.finetuning.evaluation_utils import (\n",
    "    create_benchmark_dfs_from_flux_traces,\n",
    ")\n",
    "\n",
    "\n",
    "ood_benchmark_flux_df, id_benchmark_flux_df = create_benchmark_dfs_from_flux_traces()\n",
    "ood_benchmark_data = TimeSeriesDataFrame.from_data_frame(ood_benchmark_flux_df)\n",
    "id_benchmark_data = TimeSeriesDataFrame.from_data_frame(id_benchmark_flux_df)\n",
    "ood_benchmark_data.size, id_benchmark_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(\n",
    "    data=ood_benchmark_data, extra_metrics=[\"MAE\", \"MASE\", \"MAPE\", \"SMAPE\", \"RMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.finetuning.evaluation_utils import autoregressive_forecast\n",
    "\n",
    "\n",
    "forecasts: pd.DataFrame = autoregressive_forecast(\n",
    "    benchmark_data_df=ood_benchmark_flux_df,\n",
    "    predictor=predictor,\n",
    ")\n",
    "forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.finetuning.evaluation_utils import plot_forecast_vs_true\n",
    "\n",
    "plot_forecast_vs_true(\n",
    "    benchmark_data_df=ood_benchmark_flux_df,\n",
    "    forecasts=forecasts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusiontimeseries.finetuning.evaluation_utils import evaluate_forecasts\n",
    "\n",
    "\n",
    "rsme, se_rmse = evaluate_forecasts(\n",
    "    benchmark_data_df=ood_benchmark_flux_df,\n",
    "    forecasts=forecasts,\n",
    ")\n",
    "rsme, se_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_forecasts: pd.DataFrame = autoregressive_forecast(\n",
    "    benchmark_data_df=id_benchmark_flux_df,\n",
    "    predictor=predictor,\n",
    ")\n",
    "id_forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_vs_true(\n",
    "    benchmark_data_df=id_benchmark_flux_df,\n",
    "    forecasts=id_forecasts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_rsme, id_se_rmse = evaluate_forecasts(\n",
    "    benchmark_data_df=id_benchmark_flux_df,\n",
    "    forecasts=id_forecasts,\n",
    ")\n",
    "id_rsme, id_se_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## ðŸ“Š Complete Evaluation & Results Storage\n",
    "\n",
    "This section runs a comprehensive evaluation that:\n",
    "- Generates autoregressive forecasts for both ID and OOD data\n",
    "- Computes RMSE with standard error on the prediction tails\n",
    "- Creates and saves plots for each time series\n",
    "- Saves all metadata and metrics to a JSON file in the `data/` folder\n",
    "\n",
    "The results will be saved in a format consistent with the benchmarking experiments for easy comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete evaluation with automatic saving of all results\n",
    "from fusiontimeseries.finetuning.evaluation_utils import (\n",
    "    FinetuningConfig,\n",
    "    run_complete_evaluation,\n",
    ")\n",
    "\n",
    "# Create configuration object\n",
    "finetuning_config = FinetuningConfig(\n",
    "    model_name=\"Chronos2\",\n",
    "    prediction_length=predictor.prediction_length,\n",
    "    target=\"target\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    hyperparameters=chronos2_hyperparameters,\n",
    "    time_limit=3600,\n",
    "    start_context_length=80,\n",
    "    relevant_tail_length=80,\n",
    ")\n",
    "\n",
    "# Run complete evaluation (forecasts, evaluates, plots, and saves everything)\n",
    "results, json_path, plots_dir = run_complete_evaluation(\n",
    "    predictor=predictor,\n",
    "    config=finetuning_config,\n",
    "    training_data_size=training_data.num_items,\n",
    "    predictor_path=predictor.path if hasattr(predictor, \"path\") else None,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… All results saved!\")\n",
    "print(f\"ðŸ“Š Plots directory: {plots_dir}\")\n",
    "print(f\"ðŸ“„ JSON results: {json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusiontimeseries (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
